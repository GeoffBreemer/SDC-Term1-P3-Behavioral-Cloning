'''
This code was developed for Project 3 of Udacity's Self Driving Car nanodegree program. It creates a Keras convnet
using the network architecture documented in NVIDIA's paper: https://arxiv.org/abs/1604.07316

The convnet is trained using sample data provided by Udacity, which is slightly augmented by randomly flipping and
shifting images as they are fed to the model during training. The accompanying drive.py was provided by Udacity. It
has not been changed in any way.
'''
import json                                                             # Python 3.5.2
import time
import csv
import numpy as np                                                      # v1.11.2

import matplotlib.image as img                                          # v1.5.3
import cv2                                                              # v3.1.0

from keras.preprocessing.image import flip_axis                         # Keras v1.1.1/TF v0.12.0-rc0
from keras.layers.core import SpatialDropout2D
from keras.layers import Dense, Dropout, Flatten, Lambda
from keras.layers.convolutional import Convolution2D, Cropping2D
from keras.models import Sequential
from keras.callbacks import ModelCheckpoint
from keras.optimizers import Adam

from sklearn.model_selection import train_test_split                    # 0.18.1
from sklearn.utils import shuffle

# Data import constants
LEFT_RIGHT_ANGLE = 0.08
COL_CENTER_NAME = 0
COL_LEFT_NAME = 1
COL_RIGHT_NAME = 2
COL_STEERING_ANGLE = 3

# Path constants
LOG_PATH = "./data/driving_log.csv"
IMG_PATH = "./data/"                    # note: the Udacity sample data files includes 'IMG/' in the image paths
MODEL_ARCH_PATH = 'model.json'
MODEL_WEIGHTS_PATH = 'model'

# ConvNet constants
BATCH_SIZE = 64
FLIP_PROB = 0.5
NUM_EPOCHS = 10
INITIAL_LR = 0.001

# Other constants
TRAIN_VAL_SPLIT = 0.1
RANDOM_STATE = 0
IMG_ORG_HEIGHT = 160
IMG_ORG_WIDTH = 320
X_SHIFT_RANGE = 100
Y_SHIFT_RANGE = 40


def load_data(skip_header=False, max_rows=None):
    '''
    Load images and steering angle information. Column information:
        0 = center image path
        1 = left image path
        2 = right image path
        3 = steering angle
        4 = throttle (not used)
        5 = break (not used)
        6 = speed (not used)

    :param skip_header: True if the first row contains header labels, False if there is no header
    :param max_rows: maximum number of rows to read, useful during testing to prevent loading the entire data set
    :return: (image_paths, steer_angles):
        image_paths: numpy array with image paths
        steer_angles: numpy array with steering angles
    '''
    image_paths = []
    steer_angles = []

    # Open the driving log file
    gtFile = open(LOG_PATH)
    gtReader = csv.reader(gtFile, delimiter=',')

    # The Udacity sample file contains a header row, the data file generated by the simulator does not
    if skip_header == True:
        next(gtReader)

    # Loop over all images in the log file
    for row in gtReader:
        if (len(image_paths) == max_rows):
            break

        # Store the path to the center image (i.e. not the actual image) and associated steering angle
        angle = float(row[COL_STEERING_ANGLE])
        image_paths.append(IMG_PATH + row[COL_CENTER_NAME])
        steer_angles.append(angle)

        # Also store the left and right image paths with slightly adjusted steering angles
        # to assist with faster recovery
        image_paths.append(IMG_PATH + row[COL_LEFT_NAME].strip())
        steer_angles.append(angle + LEFT_RIGHT_ANGLE)

        image_paths.append(IMG_PATH + row[COL_RIGHT_NAME].strip())
        steer_angles.append(angle - LEFT_RIGHT_ANGLE)

    gtFile.close()

    # Return the images and steering angles numpy arrays
    return np.array(image_paths), np.array(steer_angles)


def shift_image(image, steer):
    '''
    Shift an image up/down and left/right randomly.

    This code is based on code created by SDC student Vivek Yadav:
        link: https://medium.com/@vivek.yadav/using-augmentation-to-mimic-human-driving-496b569760a9#.faqcxa6ap

    :param image: image to shift
    :param steer: original steering angle associated with the image
    :return: shifted image and the steering angle updated according to the shift
    '''
    tr_x = X_SHIFT_RANGE * np.random.uniform() - X_SHIFT_RANGE / 2
    tr_y = Y_SHIFT_RANGE * np.random.uniform() - Y_SHIFT_RANGE / 2

    # Update the steering angle depending on the shift magnitude
    steer_ang = steer + tr_x / X_SHIFT_RANGE * 2 * 0.2

    # Create the transformation matrix and apply it to the image
    trans_matrix = np.float32([[1, 0, tr_x], [0, 1, tr_y]])
    image_tr = cv2.warpAffine(image, trans_matrix, (IMG_ORG_WIDTH, IMG_ORG_HEIGHT))

    return image_tr, steer_ang


def minibatch_generator(X, y, batch_size, isTraining=True):
    '''
    Generate minibatches for datasets X and y of size batch_size
    :param X: numpy array containing paths to individual images
    :param y: numpy array with steering angles associated with each image
    :param batch_size: minibatch size
    :return: generator yielding minibatches of X and y of size batch_size
    '''
    num_samples = len(y)

    while True:
        # Shuffle the data at the start of each epoch
        X, y = shuffle(X, y, random_state = RANDOM_STATE)

        # Loop through all batches covering all images exactly once
        for batch_num, pos_start in enumerate(range(0, num_samples, batch_size)):
            # Determine the true batch size of the last batch (may be less than batch_size)
            if (batch_num == num_samples // batch_size):
                true_batch_size = min(batch_num * batch_size + batch_size, num_samples) - pos_start
            else:
                true_batch_size = batch_size

            # Start with empty arrays for each batch
            batch_x = None
            batch_y = None

            # Loop through all images in the current batch
            for i in range(true_batch_size):
                image = img.imread(X[pos_start + i])
                angle = y[pos_start + i]

                # Initialize the minibatch array (faster than concatenating individual rows). The image shape is needed
                # for this; rather than hardcoding the image shape this slighty awkward approach is used instead
                if batch_x is None:
                    batch_x = np.zeros(tuple([true_batch_size] + list(image.shape)))
                    batch_y = np.zeros(true_batch_size)

                if isTraining:
                    # Randomly flip images horizontally and flip the steering angle's sign (50%/50% probability)
                    if np.random.choice([True, False], p=[FLIP_PROB, 1 - FLIP_PROB]):
                        image = flip_axis(image, 1)
                        angle *= -1

                    # Randomly translate the image to the left or right
                    image, angle = shift_image(image, angle)

                # Add the image to the mini batch
                batch_x[i] = image
                batch_y[i] = angle

            # Yield the current batch
            yield batch_x, batch_y


def normalize(image):
    '''Normalize the image to be between -0.5 and 0.5'''
    return image / 255.0 - 0.5


def resize(image):
    '''Resize the image to 66x200 as documented in the NVIDIA paper'''
    import tensorflow as tf  # This import is required here otherwise the model cannot be loaded in drive.py
    return tf.image.resize_images(image, (66, 200))


def create_convnet_nvidia():
    '''
    Create a convnet using the network architecture documented in NVIDIA's paper
    '''

    # Create the model pipeline, including image preprocessing (avoids having to change drive.py)
    model = Sequential([

        # Crop the area above the horizon, resize and normalize the image
        Cropping2D(cropping=((22, 0), (0, 0)), input_shape=(160, 320, 3)),
        Lambda(resize),
        Lambda(normalize),

        # Conv1
        Convolution2D(24, 5, 5, border_mode='valid', activation='elu', subsample=(2, 2), init="he_normal"),
        SpatialDropout2D(0.2),

        # Conv2
        Convolution2D(36, 5, 5, border_mode='valid', activation='elu', subsample=(2, 2), init="he_normal"),
        SpatialDropout2D(0.2),

        # Conv3
        Convolution2D(48, 5, 5, border_mode='valid', activation='elu', subsample=(2, 2), init="he_normal"),
        SpatialDropout2D(0.2),

        # Conv4
        Convolution2D(64, 3, 3, border_mode='valid', activation='elu', init="he_normal"),
        SpatialDropout2D(0.2),

        # Conv5
        Convolution2D(64, 3, 3, border_mode='valid', activation='elu', init="he_normal"),
        SpatialDropout2D(0.2),

        # FC1
        Flatten(),
        Dense(100, activation='elu', init="he_normal"),
        Dropout(0.5),

        # FC2
        Dense(50, activation='elu', init="he_normal"),

        # FC3
        Dense(10, activation='elu', init="he_normal"),
        Dropout(0.5),

        # Final layer
        Dense(1)
    ])

    model.summary()
    model.compile(optimizer=Adam(lr=INITIAL_LR), loss="mse")

    return model


if __name__ == "__main__":
    start_time = time.time()

    # Set Numpy random seed for reproducibility
    np.random.seed(RANDOM_STATE)

    # Load and split the data
    X_train, y_train = load_data(skip_header=True)
    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=TRAIN_VAL_SPLIT,
                                                      random_state=RANDOM_STATE)

    # Create the model
    model = create_convnet_nvidia()

    # Prepare to save the model after each epoch
    model_filepath = MODEL_WEIGHTS_PATH + "-{epoch:02d}-{val_loss:.5f}.h5"
    checkpoint = ModelCheckpoint(model_filepath, monitor='val_loss', verbose=1, save_best_only=False, mode='min')
    callbacks_list = [checkpoint]

    # Save the model; the weights are saved after each epoch
    json_string = model.to_json()
    with open(MODEL_ARCH_PATH, "w") as f:
        json.dump(json_string, f)

    # Train the model
    history = model.fit_generator(
        generator=minibatch_generator(X_train, y_train, BATCH_SIZE),
        samples_per_epoch=len(y_train),
        validation_data=minibatch_generator(X_val, y_val, BATCH_SIZE, False),
        nb_val_samples=len(y_val),
        callbacks=callbacks_list,
        nb_epoch=NUM_EPOCHS,
    )

    # Print epoch metrics
    for e in range(NUM_EPOCHS):
        print('\nEpoch {:2d}: '.format(e), end='')
        for k in sorted(history.history.keys()):
            print('{} = {:5.4f} '.format(k, history.history[k][e]), end='')
    print('\n')

    # Print total elapsed time
    print('Total elapsed time = {:<.2f} min.\n'.format((time.time() - start_time) / 60))
